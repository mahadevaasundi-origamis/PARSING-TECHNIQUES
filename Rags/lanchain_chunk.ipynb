{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691dcbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community pymupdf langchain langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ba4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "\n",
    "# load document\n",
    " \n",
    "loader = PyMuPDFLoader(\"ES Mod1@AzDOCUMENTS2.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75892c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "    text,\n",
    "    disallowed_special=()\n",
    ")\n",
    "    return len(tokens)\n",
    "tiktoken.encoding_for_model('gpt-4.1-mini')\n",
    "\n",
    "# create the length function\n",
    "token_counts = []\n",
    "for page in pages:\n",
    "    token_counts.append(tiktoken_len(page.page_content))\n",
    "min_token_count = min(token_counts)\n",
    "avg_token_count = int(sum(token_counts) / len(token_counts))\n",
    "max_token_count = max(token_counts)\n",
    "\n",
    "# print token counts\n",
    "print(f\"Min: {min_token_count}\")\n",
    "print(f\"Avg: {avg_token_count}\")\n",
    "print(f\"Max: {max_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a379dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# split documents into text and embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=1000, \n",
    "   chunk_overlap=200,\n",
    "   length_function=len,\n",
    "   is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters.nltk import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "# ensure required NLTK data is available\n",
    "nltk.download('punkt', quiet=True)\n",
    "# some NLTK versions/use-cases expect 'punkt_tab' — attempt to download if available\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except Exception:\n",
    "    # ignore if the downloader doesn't recognize this resource name\n",
    "    pass\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(\n",
    "    language='english'\n",
    ")\n",
    "\n",
    "chunks = nltk_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Ensure OPENAI_API_KEY is available. You can export it in your environment:\n",
    "#   export OPENAI_API_KEY=\"sk-...\"\n",
    "# Or enter it interactively (won't be echoed) when running this cell.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    openai_api_key = getpass(\"Enter your OpenAI API key (input hidden): \").strip()\n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    else:\n",
    "        raise ValueError(\"OpenAI API key not provided. Set OPENAI_API_KEY environment variable or provide it when prompted.\")\n",
    "\n",
    "# Initialize the embedding model with the API key\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Initialize the SemanticChunker with the embedding model\n",
    "semantic_chunker = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Split the first page into semantic chunks\n",
    "chunks = semantic_chunker.split_text(pages[0].page_content)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe972b4a",
   "metadata": {},
   "source": [
    "SemanticChunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ad3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f98e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a677da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package for Hugging Face sentence-transformers (fix ImportError)\n",
    "# %pip install -q sentence-transformers\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_core.documents import Document \n",
    "\n",
    "# Sample text for demonstration\n",
    "# text_data = \"\"\"\n",
    "# Apple's new Vision Pro headset is a spatial computing device that blends digital content with the physical world. \n",
    "# It features a micro-OLED display system and advanced eye-tracking technology.\n",
    "# The company also announced new MacBook Pro models with the M3 chip family, offering significant performance improvements for professional users.\n",
    "# In a separate development, a recent study on climate change highlights the urgent need for global cooperation. \n",
    "# Researchers suggest that renewable energy sources are crucial for mitigating rising temperatures. \n",
    "# The report emphasizes policy changes and sustainable practices.\n",
    "# \"\"\"\n",
    "\n",
    "# 1. Initialize a free embedding model from Hugging Face\n",
    "# This model runs locally and is a good default choice.\n",
    "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 2. Initialize the SemanticChunker with the embedding model\n",
    "# You can tune parameters like breakpoint_threshold_type and amount\n",
    "# to control how aggressively the text is split.\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\", # Other options: \"standard_deviation\", \"interquartile\"\n",
    "    breakpoint_threshold_amount=80 # Top 20% of semantic changes trigger a split\n",
    ")\n",
    "\n",
    "\n",
    "semantic_chunks = semantic_chunker.transform_documents(pages)\n",
    "\n",
    "\n",
    "# page1 = pages[0].page_content\n",
    "# semantic_chunks = semantic_chunker.transform_documents(pages)\n",
    "\n",
    "# print(f\"Number of semantic chunks: {len(semantic_chunks)}\")\n",
    "# print(semantic_chunks[0])\n",
    "\n",
    "\n",
    "\n",
    "# 3. Split the text into documents (chunks)\n",
    "# For a simple string, wrap it in a Document object\n",
    "# documents = [Document(page_content=text_data)]\n",
    "# semantic_chunks = semantic_chunker.split_documents(documents)\n",
    "\n",
    "# # 4. Print the resulting chunks\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"--- Chunk {i+1} (Length: {len(chunk.page_content)}) ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f509c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Example: Using Hugging Face Transformers directly to create chunk vectors (embeddings) for free\n",
    "\n",
    "\n",
    "# Choose a free, widely used sentence transformer model\n",
    "hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "hf_model = AutoModel.from_pretrained(hf_model_name)\n",
    "\n",
    "def get_chunk_embedding(text):\n",
    "    # Tokenize input text\n",
    "    inputs = hf_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = hf_model(**inputs)\n",
    "    # Mean pooling\n",
    "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# Example: create embeddings for all chunks\n",
    "chunk_vectors = [get_chunk_embedding(chunk.page_content) for chunk in semantic_chunks]\n",
    "\n",
    "print(f\"Created {len(chunk_vectors)} chunk vectors.\")\n",
    "print(chunk_vectors[0])  # Show the first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73025a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d565e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# --- Configuration (Assuming these variables are defined upstream) ---\n",
    "# NOTE: Replace 'all-MiniLM-L6-v2' with the model name defined in your hf_model_name variable\n",
    "HF_MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "\n",
    "# Initialize ChromaDB client (using default settings, stores data locally)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or get a collection for your document chunks\n",
    "# NOTE: We use the defined model name here.\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"embedded_systems_chunks\",\n",
    "    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=HF_MODEL_NAME)\n",
    ")\n",
    "\n",
    "# --- Prep and Insertion (Assuming semantic_chunks is defined and populated) ---\n",
    "# Prepare dummy data for insertion\n",
    "# documents = [\n",
    "#     \"The CPU is the brain of an embedded system, executing instructions.\",\n",
    "#     \"Microcontrollers integrate CPU, memory, and peripherals on a single chip.\",\n",
    "#     \"An RTOS manages tasks and resources in real-time constraints.\"\n",
    "# ]\n",
    "# metadatas = [\n",
    "#     {\"source\": \"textbook\", \"page\": 10},\n",
    "#     {\"source\": \"datasheet\", \"page\": 5},\n",
    "#     {\"source\": \"wiki\", \"page\": 1}\n",
    "# ]\n",
    "documents = [chunk.page_content for chunk in semantic_chunks]\n",
    "metadatas = [{\"source\": f\"page_{i+1}\"} for i in range(len(semantic_chunks))]\n",
    "ids = [f\"chunk_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Add documents and their metadata to the collection\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(documents)} chunks in ChromaDB collection 'embedded_systems_chunks'.\")\n",
    "\n",
    "# --- Retrieval with Fix (The fix for your original 'Embeddings: None' issue) ---\n",
    "\n",
    "# Explicitly include \"embeddings\" in the results for inspection\n",
    "results_with_embeddings = collection.get(\n",
    "    include=[\"metadatas\", \"documents\", \"embeddings\"] \n",
    ")\n",
    "\n",
    "ids = results_with_embeddings['ids']\n",
    "metadatas = results_with_embeddings['metadatas']\n",
    "chunk_vectors = results_with_embeddings['embeddings'] \n",
    "\n",
    "# Check if embeddings are now available (they should be, as we requested them)\n",
    "if chunk_vectors is not None and len(chunk_vectors) > 0:\n",
    "    # Print a snippet of the first vector to confirm it's not None\n",
    "    print(f\"\\n--- Retrieval Check ---\")\n",
    "    print(f\"Successfully retrieved embeddings for {len(chunk_vectors)} chunks.\")\n",
    "    print(f\"Embedding of 'chunk_0' starts with: {chunk_vectors[0][:5]}...\") \n",
    "else:\n",
    "    print(\"Error: Embeddings are still None or empty.\")\n",
    "\n",
    "# --- The Core Similarity Search (Querying the RAG system) ---\n",
    "\n",
    "# Define a query string related to the documents\n",
    "query_text = \"What is a single-chip device that controls systems?\"\n",
    "\n",
    "print(f\"\\n--- Similarity Search for: '{query_text}' ---\")\n",
    "\n",
    "# Perform the query\n",
    "# ChromaDB embeds the query_text using the same embedding function \n",
    "# and finds the most similar vectors (k=1)\n",
    "query_results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=1, \n",
    "    include=['documents', 'metadatas', 'distances'] # Only include necessary data for the result\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "if query_results and query_results.get('documents') and query_results['documents'][0]:\n",
    "    print(\"\\n✅ Closest Chunk Found:\")\n",
    "    print(f\"Document ID: {query_results['ids'][0][0]}\")\n",
    "    print(f\"Distance (Lower is better): {query_results['distances'][0][0]:.4f}\")\n",
    "    print(f\"Metadata: {query_results['metadatas'][0][0]}\")\n",
    "    print(f\"Content: **{query_results['documents'][0][0]}**\")\n",
    "else:\n",
    "    print(\"No relevant chunks found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e415acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chunk_by_query(query_text, collection, n_results=1):\n",
    "    \"\"\"\n",
    "    Query the ChromaDB collection for the most relevant chunk based on the input query text.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The text query to search for.\n",
    "        collection (chromadb.Collection): The ChromaDB collection containing document chunks.\n",
    "        n_results (int): The number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the retrieved chunk details.\n",
    "    \"\"\"\n",
    "    query_results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results,\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "    if query_results and query_results.get('documents') and query_results['documents'][0]:\n",
    "        return {\n",
    "            'id': query_results['ids'][0][0],\n",
    "            'distance': query_results['distances'][0][0],\n",
    "            'metadata': query_results['metadatas'][0][0],\n",
    "            'content': query_results['documents'][0][0]\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the query function\n",
    "# query_text = \"Briefly discuss how Cortex-M3 address the requirements of the 32-bit embedded processor market\"\n",
    "query_text = \" Briefly explain the Thumb-2 technology and its advantages over thumb instruction set with relevat diagram.\"\n",
    "\n",
    "result = query_chunk_by_query(query_text, collection)\n",
    "if result:\n",
    "    print(\"\\n✅ Query Result:\")\n",
    "    print(f\"Document ID: {result['id']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Metadata: {result['metadata']}\")\n",
    "    print(f\"Content: **{result['content']}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1d126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a80bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lanchain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
