{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db883287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santusahoo/Documents/DAGENT/dagent/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from typing import Dict, List, Tuple\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e24b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class docTRParser:\n",
    "    def __init__(self):\n",
    "        print(\"Loading docTR models...\")\n",
    "        # basic configuration\n",
    "        self.model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)\n",
    "        print(\"Models loaded successfully!\")\n",
    "\n",
    "    def extract_mixed_content(self, file_path: str, output_md_file: str = \"output.md\"):\n",
    "        \"\"\"\n",
    "        Extracts content, converting tables to HTML and keeping text as Markdown.\n",
    "        \"\"\"\n",
    "        # 1. Load Document\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            doc = DocumentFile.from_pdf(file_path)\n",
    "        else:\n",
    "            doc = DocumentFile.from_images([file_path])\n",
    "\n",
    "        # 2. Run OCR\n",
    "        print(\"Running OCR...\")\n",
    "        result = self.model(doc)\n",
    "        \n",
    "        # 3. Process & Generate Output\n",
    "        full_markdown = []\n",
    "        \n",
    "        for page_idx, page in enumerate(result.pages):\n",
    "            print(f\"Processing Page {page_idx + 1}...\")\n",
    "            full_markdown.append(f\"## Page {page_idx + 1}\\n\")\n",
    "            \n",
    "            # Analyze page to split into \"Table Blocks\" and \"Text Blocks\"\n",
    "            page_content = self._process_page_smartly(page)\n",
    "            full_markdown.append(page_content)\n",
    "            full_markdown.append(\"\\n---\\n\")\n",
    "\n",
    "        # 4. Save\n",
    "        with open(output_md_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(full_markdown))\n",
    "        print(f\"✅ Saved smart report to {output_md_file}\")\n",
    "\n",
    "    def _process_page_smartly(self, page) -> str:\n",
    "        \"\"\"\n",
    "        Intelligently separates tables from text based on column density.\n",
    "        \"\"\"\n",
    "        # A. Extract all words with coordinates\n",
    "        words_data = []\n",
    "        for block in page.blocks:\n",
    "            for line in block.lines:\n",
    "                for word in line.words:\n",
    "                    (min_x, min_y), (max_x, max_y) = word.geometry\n",
    "                    center_y = (min_y + max_y) / 2\n",
    "                    center_x = (min_x + max_x) / 2\n",
    "                    words_data.append({\n",
    "                        'text': word.value,\n",
    "                        'y': center_y,\n",
    "                        'x': center_x,\n",
    "                        'min_x': min_x, # Used for sorting\n",
    "                        'row_h': max_y - min_y\n",
    "                    })\n",
    "\n",
    "        if not words_data:\n",
    "            return \"\"\n",
    "\n",
    "        df = pd.DataFrame(words_data)\n",
    "\n",
    "        # B. Cluster into ROWS (Y-axis)\n",
    "        # 1.5% height tolerance to group words on the same line\n",
    "        y_clustering = DBSCAN(eps=0.007, min_samples=1).fit(df[['y']])\n",
    "        df['row_id'] = y_clustering.labels_\n",
    "\n",
    "        # Calculate properties for each row\n",
    "        row_stats = []\n",
    "        for rid, group in df.groupby('row_id'):\n",
    "            # Cluster X-coordinates to count \"columns\" in this specific row\n",
    "            # If items are far apart (>5% width), they are separate columns\n",
    "            x_clustering = DBSCAN(eps=0.03, min_samples=1).fit(group[['x']])\n",
    "            num_cols = len(set(x_clustering.labels_))\n",
    "            avg_y = group['y'].mean()\n",
    "            \n",
    "            row_stats.append({\n",
    "                'row_id': rid,\n",
    "                'avg_y': avg_y,\n",
    "                'num_cols': num_cols,\n",
    "                'words': group\n",
    "            })\n",
    "        \n",
    "        # Sort rows top to bottom\n",
    "        row_stats.sort(key=lambda x: x['avg_y'])\n",
    "\n",
    "        # C. Group consecutive rows into \"Content Blocks\"\n",
    "        # If a row has >= 2 columns, it's a TABLE row.\n",
    "        # If a row has 1 column, it's a TEXT row.\n",
    "        blocks = []\n",
    "        current_block = {'type': None, 'rows': []}\n",
    "\n",
    "        for row in row_stats:\n",
    "            # HEURISTIC: A row is \"Table-like\" if it has 2+ distinct columns\n",
    "            is_table_row = row['num_cols'] >= 2\n",
    "            row_type = 'table' if is_table_row else 'text'\n",
    "\n",
    "            # Start new block if type changes\n",
    "            if row_type != current_block['type']:\n",
    "                if current_block['rows']:\n",
    "                    blocks.append(current_block)\n",
    "                current_block = {'type': row_type, 'rows': [row]}\n",
    "            else:\n",
    "                current_block['rows'].append(row)\n",
    "        \n",
    "        # Append final block\n",
    "        if current_block['rows']:\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        # D. Render Blocks\n",
    "        output_str = []\n",
    "        for block in blocks:\n",
    "            if block['type'] == 'table':\n",
    "                # Convert this specific block of rows to HTML\n",
    "                html = self._rows_to_html(block['rows'])\n",
    "                output_str.append(html)\n",
    "            else:\n",
    "                # Convert this specific block of rows to Text\n",
    "                text = self._rows_to_text(block['rows'])\n",
    "                output_str.append(text)\n",
    "                \n",
    "        return \"\\n\\n\".join(output_str)\n",
    "\n",
    "    def _rows_to_html(self, rows_list) -> str:\n",
    "        \"\"\"Generates HTML table from a list of 'row' objects.\"\"\"\n",
    "        # We need to re-cluster columns GLOBALLY for this specific table block\n",
    "        # to ensure alignment (e.g., column 2 in row 1 aligns with column 2 in row 5)\n",
    "        \n",
    "        all_words_in_table = pd.concat([r['words'] for r in rows_list])\n",
    "        \n",
    "        # Cluster X-coordinates to define global columns for this table\n",
    "        x_clustering = DBSCAN(eps=0.05, min_samples=1).fit(all_words_in_table[['x']])\n",
    "        all_words_in_table['col_id'] = x_clustering.labels_\n",
    "        \n",
    "        # Sort columns left-to-right\n",
    "        col_map = all_words_in_table.groupby('col_id')['x'].mean().sort_values().reset_index()\n",
    "        col_map['sorted_col_id'] = range(len(col_map))\n",
    "        all_words_in_table = all_words_in_table.merge(col_map[['col_id', 'sorted_col_id']], on='col_id')\n",
    "\n",
    "        # Map back to specific rows (using the original row sort order)\n",
    "        # Create a mapping of original row_ids to a sorted sequence 0, 1, 2...\n",
    "        sorted_row_ids = [r['row_id'] for r in rows_list]\n",
    "        row_map = {rid: i for i, rid in enumerate(sorted_row_ids)}\n",
    "        all_words_in_table['sorted_row_id'] = all_words_in_table['row_id'].map(row_map)\n",
    "\n",
    "        # Pivot to create grid\n",
    "        grid = all_words_in_table.groupby(['sorted_row_id', 'sorted_col_id'])['text'] \\\n",
    "            .apply(lambda x: \" \".join(x)).unstack()\n",
    "        \n",
    "        # Fill empty cells\n",
    "        grid = grid.fillna(\"\")\n",
    "        \n",
    "        return grid.to_html(index=False, header=False, border=1)\n",
    "\n",
    "    def _rows_to_text(self, rows_list) -> str:\n",
    "        \"\"\"Joins text rows simply.\"\"\"\n",
    "        lines = []\n",
    "        for r in rows_list:\n",
    "            # Sort words in the line by X coordinate\n",
    "            words = r['words'].sort_values('x')\n",
    "            line_text = \" \".join(words['text'])\n",
    "            lines.append(line_text)\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b2919f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docTR models...\n",
      "Models loaded successfully!\n",
      "Running OCR...\n",
      "Processing Page 1...\n",
      "Processing Page 2...\n",
      "Processing Page 3...\n",
      "Processing Page 4...\n",
      "✅ Saved smart report to ins_po_5.md\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    processor = docTRParser()\n",
    "    # Replace with your actual file path\n",
    "    processor.extract_mixed_content(\"/Users/santusahoo/Documents/DAGENT/CRPL-1N60001074-CADPO110494.pdf\", output_md_file=\"ins_po_5.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
